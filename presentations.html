<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>Jakub Tomczak</title>
<meta http-equiv="Content-Type" content="text/html;charset=UTF-8" />
<link rel="stylesheet" type="text/css" media="screen" href="css/main.css" />
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js" type="text/javascript"></script>
<script src="js/functions.js" type="text/javascript"></script>
</head>
<body>
<div id="header">
  <ul>
    <li><a href="index.html"><span>Home</span></a></li>
    <li><a href="presentations.html"><span>Presentations</span></a></li>
    <li><a href="deebmed.html"><span>DeeBMED</span></a></li>
	<li><a href="people.html"><span>People</span></a></li>
	<li><a href="blog.html"><span>BLOG</span></a></li>
  </ul>
</div>

<!--       ABOUT       --->
<div id="about">
  <div id="story" align="justify">
    <h1>Jakub M. Tomczak</h1>
	<p>An assistant professor of Artificial Intelligence in the <a href="https://www.cs.vu.nl/ci/">Computational Intelligence group</a> (led by <a href="https://www.cs.vu.nl/~gusz/">Prof. A.E. Eiben</a>) at <a href="https://www.vu.nl/en">Vrije Universiteit Amsterdam</a>. An advisor at <a href="https://natinlab.com/">NatInLab B.V.</a>. Main research interests include deep learning, Bayesian inference and deep generative modeling.</p>
	</div>

  <ul id="contact">
    <li><img border="0" alt="mail" src="images/mail.png" width="15" height="15">&nbsp <large-text>jmk.tomczak<img border="0" alt="@" src="images/at.png" width="15" height="15">gmail.com</large-text></li>
    <li align="center"> 
<a href="https://scholar.google.pl/citations?user=XB99pR4AAAAJ&hl=en"><img border="0" alt="scholar" src="images/scholar.png" width="40" height="40"></a> &nbsp &nbsp 
<a href="https://github.com/jmtomczak"><img border="0" alt="github" src="images/github.png" width="40" height="40"></a> &nbsp &nbsp 
<a href="https://twitter.com/jmtomczak"><img border="0" alt="twitter" src="images/twitter.png" width="40" height="40"></a> &nbsp &nbsp 
<a href="https://www.linkedin.com/in/jakub-tomczak-04305314a"><img border="0" alt="linkedin" src="images/linkedin.png" width="40" height="40"></a> </li>
  </ul>
  <img id="picture" src="images/jmt.jpg" alt="" /> </div>

<!--    PRESENTATIONS    --->
<div class="section">
    <h2>Presentations</h2><br><br>
	
<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation for the CMS group, Experimental Physics Department, CERN </a>, April 20, 2022<br><b>TITLE:</b> <i>Introduction to Flow-based Generative Models</i>
		<br><b>SLIDES:</b> <a href="pdf/JTomczak_2022_04_20.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at <a href="https://www.tii.ae/seminar/ai-seminar-series-jakub-tomczak">TII AI Seminar Series</a>, March 8, 2022<br><b>TITLE:</b> <i>Deep Generative Modeling is a key to unlocking AI potential</i>
		<br><b>SLIDES:</b> <a href="pdf/JTomczak_2022_03_08.pdf" target="_blank">[here]</a>
		<br><b>VIDEO:</b> <a href="https://youtu.be/76sH8BEcwVk" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>
	
<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at <a href="https://genu.ai/2021/">GenU 2021</a>, 12-13 October 2021, Copenhagen (Denmark) <br><b>TITLE:</b> <i>Is the Likelihood-based Deep Generative Modeling appropriate for Representation Learning?</i>
		<br><b>SLIDES:</b> <a href="pdf/GenU_2021_J_Tomczak.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>
  
  
<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at beIT, 29 May 2021 <br><b>TITLE:</b> <i>Deep Generative Modeling with Variational Auto-Encoders</i>
		<br><b>SLIDES:</b> <a href="pdf/JTomczak_2021_05_29.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation for Vinted (Vilnus, Lithuania), 25 May 2021 <br><b>TITLE:</b> <i>Why AI needs Deep Generative Modeling?</i>
		<br><b>SLIDES:</b> <a href="pdf/JTomczak_2021_05_25.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at SPP Zurich 7-8 May 2021 <br><b>TITLE:</b> <i>There is no AI without Deep Generative Modelling</i>
		<br><b>SLIDES:</b> <a href="pdf/JTomczak_2021_05_8.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation for Booking.com @ Amsterdam <br><b>TITLE:</b> <i>Introduction to Deep Generative Modeling</i>
		<br><b>SLIDES:</b> <a href="pdf/JTomczak_2021_03_16.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>
  
<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at AI4Science Lab @ Univ. of Amsterdam <br><b>TITLE:</b> <i>All that glitters is not Deep Learning in Life Sciences (but sometimes it is!)</i>
		<br><b>ABSTRACT:</b> Life sciences is a fascinating field that tries to answer fundamental questions about ourselves, other species, and interactions within and among various environments. (Bio)chemistry and physics are typical tools to study and comprehend our world. However, due to the high complexity of biological systems, standard tools are not enough to understand and model all underlying relationships. Computational methods could serve as a possible remedy to that. In this talk, I will show how we can use computational intelligence, Bayesian inference, and deep learning to deal with some problems in life sciences. Specifically, we will discuss how to identify parameters in dynamical models of biological networks, how to find values of kinetic parameters in enzyme kinetics (including COVID-19), and how to count cells automatically.	
		<br><b>SLIDES:</b> <a href="pdf/All_that_glitters_is_not_DL_in_LS.pdf" target="_blank">[here]</a>
		<br><b>VIDEO:</b> <a href="https://drive.google.com/file/d/1o2WGiCQJCEe8LLyClpteeSuE8VHZ95pj/view?usp=sharing" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>


<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at Weekly Artificial Intelligence (WAI) meeting @ VU Amsterdam <br><b>TITLE:</b> <i>Why do we need deep generative modeling?</i>
		<br><b>SLIDES:</b> <a href="pdf/WAI_2020_01_13.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>An invited speech at ML in PL (Warsaw, Poland)<br><b>TITLE:</b> <i>Why do we need deep generative modeling?</i><br><b>ABSTRACT:</b> Deep learning achieves state-of-the-art results in tasks like image or audio classification. However, adding noise to data can easily fool a deep learning model. During this talk, we will discuss a possible remedy to this issue, namely, learning generative models. We will start with a motivating example of image classification and highlight that training a joint distribution over a label and an object (image) is crucial for uncertainty quantification. Next, we will outline different approaches to model a distribution over objects (e.g., images). More specifically, we will focus on Variational Auto-Encoders and Flow-based models, which are models that allow to learn (approximate) probability distributions. In the conclusion, we will show successes and failures of these models, indicating possible future research directions.<br><b>SLIDES:</b> <a href="pdf/MLinPL_2019_11_24.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at AwesomeIT (Amsterdam, the Netherlands)<br><b>TITLE:</b> <i>The Future of Deep Learning: Deep Generative Modeling</i><br><b>ABSTRACT:</b> Deep learning has become almost a default tool in many real-life problems like image analysis, audo analysis and text analysis. Due to increasing computational capabilities, neural networks are deeper and their training is faster. However, learning models that are capable of capturing rich distributions from vast amounts of (unlabeled) data remains one of the major challenges of artificial intelligence. For instance, there is an enormous number of images available online, however, labeling them all is almost an impossible task. Therefore, in order to take advantage of a flood of unlabeled data, deep generative modeling provides a natural manner of dealing with both labeled and unlabeled data.<br>
In recent years, different approaches to deep generative modeling were proposed by formulating alternative training objectives to the log-likelihood like the adversarial loss that leads to Generative Adversarial Networks (GANs) or by utilizing variational inference that results in a family of Variational Auto-Encoders (VAE). A third way is an application of autoregressive models like PixelCNN or WaveNet. During our meeting we will discuss these three approaches and point out their advatanges and disadvantages. In order to present their successes, the most promising applications of these deep generative models will be outlined.<br><b>SLIDES:</b> <a href="pdf/AwesomeIT_2019_04_5.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation at Summer School on Data Science (Split, Croatia)<br><b>TITLE:</b> <i>Deep Generative Models: GANs and VAE</i><br><b>ABSTRACT:</b> During this talk I present why generative modeling is important and what are the main trends in generative modeling. I focus on Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAE). I outline basics of these two approaches and point to recent developments. Moreover, I provice pros and cons of these methods.<br><b>SLIDES:</b> <a href="pdf/ssds2017.pdf" target="_blank">[here]</a></li>
	<br>
	<li>A presentation at Technische Universiteit Eindhoven<br><b>TITLE:</b> <i>Variational Auto-Encoder: Deep Learning meets Generative Modeling</i><br><b>ABSTRACT:</b> Variational auto-encoder (VAE) is a scalable and powerful generative framework. The main advantage of the VAE is that it allows to model stochastic dependencies between random variables using deep neural networks that can be further trained by gradient-based methods (backpropagation). There are three main components within the VAE framework: (i) a decoder, (ii) an encoder, (iii) a prior. During the talk I will introduce basic ideas of the VAE and show how these three components could be formulated. I will especially focus on increasing flexibility of the encoder using the idea of normalizing flows. Further, I will present how to choose the prior for learning better latent representation. Eventually, I will outline possible extensions and future directions.<br><b>SLIDES:</b> <a href="pdf/tue_06_2017.pdf" target="_blank">[here]</a></li>
	</p>
    </div>
  </div>

<div class="item">
    <div class="description" align="justify">
      <p><ul>
	<li>A presentation in CWI (Dept. of Life Sciences and Health)<br><b>TITLE:</b> <i>Deep Generative Modeling using Variational Auto-Encoders</i><br><b>ABSTRACT:</b> Learning generative models that are capable of capturing rich distributions from vast amounts of data like image collections remains one of the major challenges of artificial intelligence. In recent years, different approaches to achieve this goal were proposed by formulating alternative training objectives to the log-likelihood like the adversarial loss or by utilizing variational inference. The latter approach could be made especially efficient through the application of the reparameterization trick resulting in a highly scalable framework now known as the variational auto-encoders (VAE). VAEs are scalable and powerful generative models that can be easily utilized in any probabilistic framework. The tractability and the flexibility of the VAE follow from the choice of the variational posterior (the encoder), the prior over latent variables and the decoder. 
<br>
In this presentation I will outline different manners of improving the VAE. Moreover, I will discuss current applications and possible future directions.<br><b>SLIDES:</b> <a href="pdf/CWI_2018_05_29.pdf" target="_blank">[here]</a></li>
	<br>
	<li><b>Jakub M. Tomczak</b>, an oral presentation at AISTATS (the Canary Islands)<br><b>TITLE:</b> <i>VAE with a VampPrior</i><br><b>ABSTRACT:</b> Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.<br><b>SLIDES:</b> <a href="pdf/AISTATS_2018.pdf" target="_blank">[here]</a></li>
	</ul></p>
    </div>
   </div>

  <div class="item">
    <div class="description" align="justify">
      <p><ul>
      <li>A presentation at PASC 2018 Conference (2nd of July, 2018) and CERN (3rd of July, 2018) <br><b>TITLE:</b> <i>The Success of Deep Generative Models</i><br><b>ABSTRACT:</b> Deep generative models allow to learn hidden representation of data and generate new examples. There are two major families of models that are exploited in current applications : Generative Adversarial Networks (GANs), and Variational Auto-Encoders (VAE). The principle of GANs is to train a generator that can generate examples from random noise, in adversary of a discrimanitive model that is forced to confused true samples from generated ones. Generated images by GANs are very sharp and detailed. The biggest disadvantage of GANs is that they are trained through solving a minimax optimization problem that causes significant learning instability issues. VAEs are based on a fully probabilistic perspective of the variational inference. The learning problem aims at maximizing the variational lower bound for a given family of variational posteriors. The model can be trained by backpropagation but it was noticed that the resulting generated images are rather blurry. However, VAEs are probabilistic models, thus, they could be incorporated in almost any probabilistic framework. We will discuss basics of both approaches and present recent extensions. We will point out advantages and disadvantages of GANs and VAE. Some of most promising applications of deep generative models will be shown.<br>
<b>ANNOUNCMENT at CERN</b>: <a href="https://indico.cern.ch/event/736998/" target="_blank">[link]</a>
<br>
<b>SLIDES:</b> <a href="pdf/PASC_2018_07_2.pdf" target="_blank">[PASC]</a>, <a href="pdf/CERN_2018_07_3.pdf" target="_blank">[CERN]</a>
<br>
<b>VIDEO</b>: <a href="https://webcast.web.cern.ch/event/i736998" target="_blank">[link]</a></li>
	<br>
	<li>A presentation for Tooploox company <br><b>TITLE:</b> <i>Attention-based Deep Multiple Instance Learning</i><br><b>ABSTRACT:</b> Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.<br><b>SLIDES:</b> <a href="pdf/Tooploox_2018_06_27.pdf" target="_blank">[here]</a></li>
	</ul></p>
   </div>
  </div>
</div>
<div id="footer">
  <p>&copy; Jakub Tomczak. All rights reserved.</p>
  <!-- Don't remove this link. Respect the designer. If you want to remove it, shoot me an email me@jonnotie.nl -->
</div>
</body>
</html>
