<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>2_ARM</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<!-- MUST INCLUDE TO PROPERLY ADD HEADER! -->
<link rel="stylesheet" type="text/css" media="screen" href="../../css/main.css" />
<link rel="stylesheet" type="text/css" media="screen" href="../blog.css" />

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div id="header">
  <ul>
    <li><a href="../../index.html"><span>Home</span></a></li>
    <li><a href="../../presentations.html"><span>Presentations</span></a></li>
    <li><a href="../../deebmed.html"><span>DeeBMED</span></a></li>
	<li><a href="../../people.html"><span>People</span></a></li>
	<li><a href="../../blog.html"><span>BLOG</span></a></li>
  </ul>
</div>

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Flow-based-Models">Flow-based Models<a class="anchor-link" href="#Flow-based-Models">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So far, we have discussed a class of deep generative models that model the distribution $p(\mathbf{x})$ directly in an autoregressive manner. The main advantage of ARMs is that they can learn long-range statistics and, in a consequence, powerful density estimators. However, their drawback is that they are parameterized in an autoregressive manner, hence, sampling is rather a slow process. Moreover, they lack a latent representation, therefore, it's not obvious how to manipulate their internal data representation that makes it less appealing for tasks like lossy compression or metric learning.</p>
<p>In this write-up, we present a different approach to direct modeling of $p(\mathbf{x})$. Before we start our considerations, we will discuss a simple example.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Example</strong>
Let us take a random variable $z \in \mathbb{R}$ with $\pi(z) = \mathcal{N}(z|0,1)$. Now, we consider a new random variable after applying some linear transformation to $z$, namely, $x = 0.75 z + 1$. Now the question is the following: What is the distribution of $x$, $p(x)$? We can guess the solution by using properties of Gaussians, or dig in our memory about the <strong>change of variables formula</strong> to calculate this distribution, that is:</p>
$$
p(x) = \pi\left(z = f^{-1}(x)\right) \left|\frac{\partial f^{-1}(x)}{\partial x} \right| ,
$$<p>where $f$ is an invertible function (a bijection). What does it mean? It means that the function maps one point to another, distinctive point, and we can always invert the function to obtain the original point.</p>
<p><img src="bijection.png" width="200"></p>
<p>In the figure above, we have a very simple example of a bijection. Notice that volumes of the domains do not need to be the same! Keep it in mind and think about it in the context of $\left|\frac{\partial f^{-1}(x)}{\partial x} \right|$.</p>
<p>Coming back to our example, we have:</p>
$$
f(z) = 0.75 z + 1, 
$$<p>and the inverse of $f$ is:</p>
$$
f^{-1}(x) = \frac{x - 1}{0.75} .
$$<p>Then, the derivative of the <strong>change of volume</strong> is:</p>
$$
\left| \frac{\partial f^{-1}(x)}{\partial x} \right| = \frac{4}{3} .
$$<p>Putting all together yields:</p>
$$
p(x) = \pi\left( z = \frac{x - 1}{0.75} \right) \frac{4}{3} = \frac{1}{\sqrt{2 \pi\ 0.75^2} } \exp \left\{ - (x - 1)^2/0.75^2 \right\}
$$<p>We immediately realize that we end up with the Gaussian distribution again:</p>
$$
p(x) = \mathcal{N}(x|1, 0.75).
$$<p>Moreover, we see that the part $\left|\frac{\partial f^{-1}(x)}{\partial x} \right|$ is responsible to <strong>normalize</strong> the distribution $\pi(z)$ after applying the transformation $f$. In other words, $\left|\frac{\partial f^{-1}(x)}{\partial x} \right|$ counteracts a possible <em>change of volume</em> caused by $f$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First of all, this example indicates that we can calculate a new distribution of a continuous random variable by applying a known bijective transformation $f$ to a random variable with a known distribution, $z \sim p(z)$. The same holds for multiple variables $\mathbf{x}, \mathbf{z} \in \mathbb{R}^{D}$:</p>
\begin{equation*}
p(\mathbf{x}) = p\left(\mathbf{z}=f^{-1}(\mathbf{x})\right) \left|\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right| ,
\label{eq:change_of_variables} \tag{1}
\end{equation*}<p>where:</p>
$$
\left|\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right| = \left| \det \mathbf{J}_{f^{-1}}(\mathbf{x})\right|
$$<p>is the Jacobian matrix $\mathbf{J}_{f^{-1}}$ that is defined as follows:</p>
$$
\mathbf{J}_{f^{-1}} =\left[\begin{array}{ccc}
\frac{\partial f_{1}^{-1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{1}^{-1}}{\partial x_{D}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_{D}^{-1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{D}^{-1}}{\partial x_{D}}
\end{array}\right] .
$$<p>Moreover, we can also use the <strong>inverse function theorem</strong> that yields:</p>
$$
\left| \mathbf{J}_{f^{-1}}(\mathbf{x}) \right| = \left| \mathbf{J}_{f}(\mathbf{x}) \right|^{-1} .
$$<p>Since $f$ is invertible, we can use the inverse function theorem to rewrite (\ref{eq:change_of_variables}) as follows:</p>
$$
p(\mathbf{x}) = p\left(\mathbf{z}=f^{-1}(\mathbf{x})\right) \left| \mathbf{J}_{f}(\mathbf{x}) \right|^{-1} .
%\label{eq:change_of_variables2} \tag{2}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To get some insight into the role of the Jacobian-determinant, take a look at Figure 1. Here, there are three cases of invertible transformations that play around with a uniform distribution defined over a square.</p>
<p><img src="jacobian_det.png" width="400"></p>
<p><strong>Figure 1.</strong> Three examples of invertible transformations: (<em>top</em>) a volume-preserving bijection, (<em>middle</em>) a bijection that shrinks the original area, (<em>bottom</em>) a bijection that enlarges the original area.</p>
<p>In the case on top, the transformation turns a square into a rhombus without changing its volume. As a result, the Jacobian-determinant of this transformation is $1$. Such transformations are called <strong>volume-preserving</strong>. Notice that the resulting distribution is still uniform and since there is no change of volume, it is defined over the same volume as the original one, thus, the color is the same.</p>
<p>In the middle, the transformation shrinks the volume, therefore, the resulting uniform distribution is "denser" (a darker color in Figure 1). Additionally, the Jacobian-determinant is smaller than $1$.</p>
<p>In the last situation, the transformation enlarges the volume, hence, the uniform distribution is defined over a larger area (a lighter color in Figure 1). Since the volume is larger, the Jacobian-determinant is larger than $1$.</p>
<p>Notice that shifting operator is volume-preserving. To see that imagine adding an arbitrary value (e.g., $5$) to all points of the square. Does it change the volume? Not at all! Thus, the Jacobian-determinant equals $1$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Change-of-variables-for-deep-generative-modeling">Change of variables for deep generative modeling<a class="anchor-link" href="#Change-of-variables-for-deep-generative-modeling">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A natural question is whether we can utilize the idea of the change of variables to model a complex and high-dimensional distribution over images, audio or other data sources. Let us consider a hierarchical model, or, equivalently, a sequence of invertible transformations, $f_{k}: \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$. We start with a known distribution $\pi(\mathbf{z}_{0}) = \mathcal{N}(\mathbf{z}_{0} | 0, \mathbf{I})$. Then, we can sequentially apply the invertible transformations to obtain a flexible distribution (Rezende &amp; Mohamed, 2015; Rippel &amp; Adams, 2013):</p>
$$
p(\mathbf{x})=\pi\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) \right) \prod_{i=1}^{K}\left|\operatorname{det} \frac{\partial f_{i}\left(\mathbf{z}_{i-1}\right)}{\partial \mathbf{z}_{i-1}}\right|^{-1}
$$<p>or by using the notation of a Jacobian for the $i$-th transformation:</p>
$$
p(\mathbf{x}) = \pi\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) \right) \prod_{i=1}^{K} \left| \mathbf{J}_{f_{i}}(\mathbf{z}_{i-1}) \right|^{-1} .
$$<p>An example of transforming a unimodal base distribution like Gaussian into a multimodal distribution through invertible transformations is presented in Figure 2. In principle, we should be able to get almost arbitrary comples distribution and revert to a "simple" one.</p>
<p><img src="flow.png" width="700"></p>
<p><strong>Figure 2.</strong> An example of transforming a unimodal distribution (the latent space) to a multimodal distribution (the data space, e.g., the pixel space) through a series of invertible transformations $f_i$.</p>
<p>Let $\pi(\mathbf{z}_{0})$ be $\mathcal{N}(\mathbf{z}_0 | 0, \mathbf{I})$. Then, the logarithm of $p(\mathbf{x})$ is the following:
\begin{equation*}
\ln p(\mathbf{x}) = \ln \mathcal{N}\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) | 0, \mathbf{I} \right) - \sum_{i=1}^{K} \ln \left| \mathbf{J}_{f_{i}}(\mathbf{z}_{i-1}) \right| .
\label{eq:final_change_of_variables} \tag{2}
\end{equation*}</p>
<p>Interestingly, we see that the first part, namely, $\ln \mathcal{N}\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) | 0, \mathbf{I} \right)$ corresponds to the <em>Mean Square Error</em> loss function between $0$ and $f^{-1}(\mathbf{x})$. The second part, $\sum_{i=1}^{K} \ln \left| \mathbf{J}_{f_{i}}(\mathbf{z}_{i-1}) \right|$, as in our example, ensures that the distribution is properly normalized. However, since it penalizes the change of volume (take a look again at the example above!), we can think of it as a kind of a <em>regularizer</em> for the invertible transformations $\{f_i\}$.</p>
<p>Once we have laid down the foundations of the change of variables for expressing density functions, now we must face two questions:</p>
<ul>
<li>How to model the invertible transformations?</li>
<li>What is the difficulty here?</li>
</ul>
<p>The answer to the first question could be <strong>neural networks</strong> because they are flexible and easy-to-train. However, we cannot take <strong>any</strong> neural network because of two reasons. First, the transformation must be <strong>invertible</strong>, thus, we must pick an <strong>invertible neural network</strong>. Second, even if a neural network is invertible, we face a problem of calculating the second part of (\ref{eq:final_change_of<em>variables}), i.e., $\sum</em>{i=1}^{K} \ln \left| \mathbf{J}<em>{f</em>{i}}(\mathbf{z}_{i-1}) \right|$, that is non-trivial and computationally intractable for an arbitrary sequence of invertible transformations. As a result, we seek for such neural networks that are both invertible and the logarithm of a Jacobian-determinant is (relatively) easy to calculate. The resulting model that consists of invertible transformations (neural networks) with tractable Jacobian-determinants are referred to as <strong>normalizing flows</strong> or <strong>flow-based models</strong>.</p>
<p>There are various possible invertible neural networks with tractable Jacobian-determinants, e.g., Planar Normalizing Flows (Rezende &amp; Mohamed, 2015), Sylvester Normalizing Flows (van den Berg et al., 2018), Residual Flows (Behrmann et al., 2019; Chen et al., 2019), Invertible DenseNets (Perugachi-Diaz et al., 2021). However, here we focus on a very important class of models: <strong>RealNVP</strong>, <em>Real-valued Non-Volume Preserving</em> flows (Dinh et al., 2016).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Coupling-layers,-permutation-layers-and-dequantization">Coupling layers, permutation layers and dequantization<a class="anchor-link" href="#Coupling-layers,-permutation-layers-and-dequantization">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong><em>Coupling layers</em></strong> The main component of RealNVP is a <strong>coupling layer</strong>. The idea behind this transformation is the followng. Let us consider an input to the layer that is divided into two parts: $\mathbf{x} = [\mathbf{x}_{a}, \mathbf{x}_{b}]$. The division into two parts could be done by dividing the vector $\mathbf{x}$ into $\mathbf{x}_{1:d}$ and $\mathbf{x}_{d+1:D}$ or according to a more sophisticated manner, e.g., a <em>checkerboard pattern</em> (Dinh et al., 2016). Then, the transformation is defined as follows:</p>
\begin{align*}
\mathbf{y}_{a} &amp;= \mathbf{x}_{a} \\
\mathbf{y}_{b} &amp;= \exp \left(s\left(\mathbf{x}_{a}\right)\right) \odot \mathbf{x}_{b} + t\left(\mathbf{x}_{a}\right) ,
\end{align*}<p>where $s(\cdot)$ and $t(\cdot)$ are <strong>arbitrary neural networks</strong> called <em>scaling</em> and <em>transition</em>, respectively.</p>
<p>This transformation is invertible by design, namely:
\begin{align*}
\mathbf{x}_{b} &amp;= \left(\mathbf{y}_{b} - t(\mathbf{y}_{a})\right) \odot \exp \left(-s(\mathbf{y}_{a})\right) \\
\mathbf{x}_{a} &amp;= \mathbf{y}_{a} .
\end{align*}</p>
<p>Importantly, the logarithm of the Jacobian-determinant is easy-to-calculate, because:</p>
$$
\mathbf{J}=\left[\begin{array}{cc}
\mathbf{I}_{d\times d} &amp; \mathbf{0}_{d \times(D-d)} \\
\frac{\partial \mathbf{y}_{b}}{\partial \mathbf{x}_{a}} &amp; \operatorname{diag}\left(\exp \left(s\left(\mathbf{x}_{a}\right)\right)\right)
\end{array}\right]
$$<p>that yields:
$$
\det(\mathbf{J})=\prod_{j=1}^{D-d} \exp \left(s\left(\mathbf{x}_{a}\right)\right)_{j}=\exp \left(\sum_{j=1}^{D-d} s\left(\mathbf{x}_{a}\right)_{j}\right) .
$$</p>
<p>Eventually, coupling layers seem to be flexible and powerful transformations with tractable Jacobian-determinants! However, we process only half of the input, therefore, we must think of an appropriate additional transformation to be combined with.</p>
<p><strong><em>Permutation layer</em></strong> A simple yet effective transformation that could be combined with a coupling layer is a <strong>permutation layer</strong>. Since permutation is <strong>volume-preserving</strong>, i.e., its Jacobian-determinant is equal to $1$, we can apply it each time after the coupling layer. For instance, we can reverse the order of variables.</p>
<p>An example of an invertible block, i.e., a combination of a coupling layer with a permutation layer is schematically presented in Figure 3.</p>
<p><img src="coupling_permutation_block.png" width="800"></p>
<p><strong>Figure 3.</strong> A combination of a coupling layer and a permutation layer that transforms $[\mathbf{x}_{a}, \mathbf{x}_{b}]$ to $[\mathbf{z}_{a}, \mathbf{z}_{b}]$. <strong>A</strong> A forward pass through the block. <strong>B</strong> An inverse pass through the block.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong><em>Dequantization</em></strong> As discussed so far, flow-based models assume that $\mathbf{x}$ is a vector of real-valued random variables. However, in practice, many objects are discrete. For instance, images are typically represented as integers taking values in $\{0, 1, ..., 255\}^{D}$. In (Theis et al., 2016), it has been outlined that adding a uniform noise, $\mathbf{u} \in [-0.5,0.5]^{D}$, to original data, $\mathbf{y} \in \{0, 1, ..., 255\}^{D}$, allows applying density estimation to $\mathbf{x} = \mathbf{y} + \mathbf{u}$. This procedure is known as <strong>uniform dequantization</strong>. Recently, there were different schema of dequantization proposed, you can read more on that in (Hoogeboom et al., 2021).</p>
<p>An example for two binary random variables and the uniform dequantization is depicted in Figure 4. After adding $\mathbf{u} \in [-0.5,0.5]^{2}$ to each discrete value, we obtain a continuous space and now probabilities originally associated with volumeless points are "spread" across small square regions.</p>
<p><img src="uniform_dequantization_2d.png" width="400"></p>
<p><strong>Figure 4.</strong> A schematic representation of the uniform dequantization for two binary random variables: (<em>left</em>) the probability mass is assigned to points, (<em>right</em>) after the uniform dequantization, the probability mass is assigned to square areas. Colors correspond to probability values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Flows-in-action!">Flows in action!<a class="anchor-link" href="#Flows-in-action!">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us turn math into a code! We will first discuss the log-likelihood function (i.e., the learning objective) and how mathematical formulas correspond to the code. For the full code (with auxiliary functions etc.), please take a look at the additional file available here: <a href="https://github.com/jmtomczak/intro_dgm" target="_blank">[link]</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, it is extremely important to know what is our learning objective, i.e., the log-likelihood function. In the example, we use coupling layers as described earlier, together with permutation layers. Then, we can plug the logarithm of the Jacobian-determinant for the coupling layers (for the permulation layers it is equal to $1$, so $\ln (1) = 0$) in (2) that yields:</p>
$$
\ln p(\mathbf{x}) = \ln \mathcal{N}\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) | 0, \mathbf{I} \right) - \sum_{i=1}^{K} \left(\sum_{j=1}^{D-d} s_{k}\left(\mathbf{x}_{a}^{k}\right)_{j} \right),
$$<p>where $s_{k}$ is the scale network in the $k$-th coupling layer, and $\mathbf{x}_{a}^{k}$ denotes the input to the $k$-th coupling layer. Notice that $\exp$ in the log-Jacobian-determinant is cancelled by applying $\ln$.</p>
<p>Let us think again about the learning objective from the implementation perspective. First, we definitely need to obtain $\mathbf{z}$ by calculating $f^{-1}(\mathbf{x})$, and then we can calculate $\ln \mathcal{N}\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) | 0, \mathbf{I} \right)$. That's actually easy, and we get:</p>
$$
\ln \mathcal{N}\left(\mathbf{z}_{0} = f^{-1}(\mathbf{x}) | 0, \mathbf{I} \right) = -\text{const} - \frac{1}{2}\|f^{-1}(\mathbf{x})\|^{2}
$$<p>where $\text{const} = \frac{D}{2} \ln \left( 2\pi \right)$ is the normalizing constant of the standard Gaussian, and $\frac{1}{2}\|f^{-1}(\mathbf{x})\|^{2} = MSE(0, f^{-1}(\mathbf{x}))$.</p>
<p>Alright, now we should look into the second part of the objective, i.e., the log-Jacobian-determinants. As we can see, we have a sum over transformations, and for each coupling layer, we consider only the outputs of the scale nets. Hence, the only thing we must remember during implementing the coupling layers is to return not only output but also the outcome of the scale layer too.</p>
<p>Now, we have all components to implement our own RealNVP! Below, there is a code with a lot of comments that should help to understand every single line of it. The full code (with auxiliary functions) that you can play with is available here: <a href="https://github.com/jmtomczak/intro_dgm" target="_blank">[link]</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">RealNVP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nets</span><span class="p">,</span> <span class="n">nett</span><span class="p">,</span> <span class="n">num_flows</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dequantization</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RealNVP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Well, it&#39;s always good to brag about yourself.</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RealNVP by JT.&#39;</span><span class="p">)</span>
        
        <span class="c1"># We need to dequantize discrete data. This attribute is used during training to dequantize integer data.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequantization</span> <span class="o">=</span> <span class="n">dequantization</span>
        
        <span class="c1"># An object of a prior (here: torch.distribution of multivariate normal distribution)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="c1"># A module list for translation networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nett</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_flows</span><span class="p">)])</span>
        <span class="c1"># A module list for scale networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nets</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_flows</span><span class="p">)])</span>
        <span class="c1"># The number of transformations, in our equations it is denoted by K.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_flows</span> <span class="o">=</span> <span class="n">num_flows</span>
        
        <span class="c1"># The dimensionality of the input. It is used for sampling.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>

    <span class="c1"># This is the coupling layer, the core of the RealNVP model.</span>
    <span class="k">def</span> <span class="nf">coupling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># x: input, either images (for the first transformation) or outputs from the previous transformation</span>
        <span class="c1"># index: it determines the index of the transformation</span>
        <span class="c1"># forward: whether it is a pass from x to y (forward=True), or from y to x (forward=False)</span>
        
        <span class="c1"># We chunk the input into two parts: x_a, x_b</span>
        <span class="p">(</span><span class="n">xa</span><span class="p">,</span> <span class="n">xb</span><span class="p">)</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># We calculate s(xa), but without exp!</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">index</span><span class="p">](</span><span class="n">xa</span><span class="p">)</span>
        <span class="c1"># We calculate t(xa)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">[</span><span class="n">index</span><span class="p">](</span><span class="n">xa</span><span class="p">)</span>
        
        <span class="c1"># Calculate either the forward pass (x -&gt; z) or the inverse pass (z -&gt; x)</span>
        <span class="c1"># Note that we use the exp here!</span>
        <span class="k">if</span> <span class="n">forward</span><span class="p">:</span>
            <span class="c1">#yb = f^{-1}(x)</span>
            <span class="n">yb</span> <span class="o">=</span> <span class="p">(</span><span class="n">xb</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#xb = f(y)</span>
            <span class="n">yb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">xb</span> <span class="o">+</span> <span class="n">t</span>
        
        <span class="c1"># We return the output y = [ya, yb], but also s for calculating the log-Jacobian-determinant</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">xa</span><span class="p">,</span> <span class="n">yb</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">s</span>

    <span class="c1"># An implementation of the permutation layer</span>
    <span class="k">def</span> <span class="nf">permute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Simply flip the order.</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># This is a function that calculates the full forward pass through the coupling+permutation layers.</span>
        <span class="c1"># We initialize the log-Jacobian-det</span>
        <span class="n">log_det_J</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span>
        <span class="c1"># We iterate through all layers</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_flows</span><span class="p">):</span>
            <span class="c1"># First, do coupling layer,</span>
            <span class="n">z</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coupling</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># then permute.</span>
            <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="c1"># To calculate the log-Jacobian-determinant of the sequence of transformations we sum over all of them.</span>
            <span class="c1"># As a result, we can simply accumulate individual log-Jacobian determinants.</span>
            <span class="n">log_det_J</span> <span class="o">=</span> <span class="n">log_det_J</span> <span class="o">-</span> <span class="n">s</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># We return both z and the log-Jacobian-determinant, because we need z to feed in to the logarightm of the Norma;</span>
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span>

    <span class="k">def</span> <span class="nf">f_inv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># The inverse path: from z to x.</span>
        <span class="c1"># We appply all transformations in the reversed order.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">z</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_flows</span><span class="p">)):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coupling</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Since we use this function for sampling, we don&#39;t need to return anything else than x.</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;avg&#39;</span><span class="p">):</span>
        <span class="c1"># This function is essential for PyTorch.</span>
        <span class="c1"># First, we calculate the forward part: from x to z, and also we need the log-Jacobian-determinant.</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># We can use either sum or average as the output.</span>
        <span class="c1"># Either way, we calculate the learning objective: self.prior.log_prob(z) + log_det_J.</span>
        <span class="c1"># NOTE: Mind the minus sign! We need it, because, by default, we consider the minimization problem,</span>
        <span class="c1"># but normally we look for the maximum likelihood estimate. Therefore, we use:</span>
        <span class="c1"># max F(x) &lt;=&gt; min -F(x)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_det_J</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_det_J</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">):</span>
        <span class="c1"># First, we sample from the prior, z ~ p(z) = Normal(z|0,1)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">))</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="c1"># Second, we go from z to x.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_inv</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># The number of flows</span>
<span class="n">num_flows</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># Neural networks for a single transformation (a single flow).</span>
<span class="n">nets</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">())</span>

<span class="n">nett</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># For the prior, we can use the built-in PyTorch distribution.</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>

<span class="c1"># Init of the RealNVP. Please note that we need to dequantize the data (i.e., uniform dequantization).</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RealNVP</span><span class="p">(</span><span class="n">nets</span><span class="p">,</span> <span class="n">nett</span><span class="p">,</span> <span class="n">num_flows</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">dequantization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Viola! Here we go, this is all we need to have. After running the code (take a look at: <a href="https://github.com/jmtomczak/intro_dgm" target="_blank">[link]</a>) and training the RealNVP, we should obtain results similar to the following:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>A</strong> <img src="realnvp_real_images.png" width="200"> 
<strong>B</strong> <img src="realnvp_generated_images.png" width="200">
<strong>C</strong> <img src="realnvp_nll_val_curve.png" width="300"></p>
<p><strong>Figure 5.</strong> Examples of outcomes of the training:
<strong>A</strong> Randomly selected real images.
<strong>B</strong> Unconditional generations from the RealNVP.
<strong>C</strong> The validation curve during training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Is-it-all?-Really?">Is it all? Really?<a class="anchor-link" href="#Is-it-all?-Really?">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Yes and no. Yes in the sense it is the minimalistic example of an implementation of the RealNVP. No, because there are many <strong>improvements</strong>:</p>
<p><em>Factoring out</em> (Dinh et al., 2016): During the forward pass (from $\mathbf{x}$ to $\mathbf{z}$), we can split the variables and proceed with processing only a subset of them. This could help to parameterize the base distribution by using the outputs of intermediate layers.</p>
<p><em>Rezero trick</em> (Bachlechner et al., 2020): Introducing additional parameters to the coupling layer, e.g., $\mathbf{y}_{b} = \exp\left(\alpha s(\mathbf{x}_{a})\right) \odot \mathbf{x}_{b} + \beta t(\mathbf{x}_{a})$ and $\alpha, \beta$ are initilized with $0$'s. This helps to ensure that the transformations act as identity maps in the beginning. It is shown in (Bachlechner et al., 2020) that it helps to learn better transformations by maintaining information about the input through all layers in the beginning of training.</p>
<p><em>Masking</em> or <em>Checkerboard pattern</em> (Dinh et al., 2016): We can use a checkerboard pattern instead of dividing an input into two parts like $[\mathbf{x}_{1:D/2}, \mathbf{x}_{D/2+1:D}]$. This encourages learning local statistics better.</p>
<p><em>Squeezing</em> (Dinh et al., 2016): We can also play around with "squeezing" some dimensions. For instance, an image consists of <em>C</em> channels, width <em>W</em>, and height <em>H</em>, could be turned into <em>4C</em> channels, width <em>W/2</em> and height <em>H/2</em>.</p>
<p><em>Learnable base distributions</em> : instead of using a standard Gaussian base distribution, we can use another model for that, e.g., an autoregressive model.</p>
<p><em>Invertible 1x1 convolution</em> (Kingma &amp; Dhariwal, 2018): A fixed permutation could be replaced with a (learned) invertible 1x1 convolution as in the GLOW model of Kingma &amp; Dhariwal.</p>
<p><em>Variational dequantization</em> (Ho et al., 2019a): We can also pick a different dequantization scheme, e.g., variational dequantization. This allows to obtain much better scored. However, it's not for free because it leads to a lower bound to the log-likelihood function.</p>
<p>Moreover, there are many <strong>new fascinating research directions</strong>! I will name them here and point to papers where you can find more details:</p>
<p><em>Compression with flows</em> (Ho et al., 2019b): Flow-based models are perfect candidates for compression since they allow to calculate the exact likelihood. Ho et al. proposed a scheme that allows to use flows in the bit-back-like compression scheme.</p>
<p><em>Conditional flows</em> (Stypulkowski et al., 2020, Winkler et al., 2019; Wolf et al., 2021): Here, we present the unconditional RealNVP. However, we can used a flow-based model for conditional distributions. For instance, we can use the conditioning as an input to the scale network and the translation network.</p>
<p><em>Variational inference with flows</em> (van den Berg et al., 2018; Kingma et al., 2016; Rezende &amp; Mohamed, 2015; Hoogeboom et al., 2021; Tomczak &amp; Welling, 2016; Tomczak &amp; Welling, 2017): Conditional flow-based models could be used to form a flexible family of variational posteriors. The, the lower-bound to the log-likelihood function should be tighter.</p>
<p><em>Integer discrete flows</em> (Hoogeboom et al., 2019; van den Berg et al., 2020; Tomczak, 2020): Another interesting direction is a version of the RealNVP for integer-valued data. We will explain it in another blog post.</p>
<p><em>Flows on manifolds</em> (Brehmer &amp; Cranmer, 2020): Typically, flow-based models are considered in the Euclidean space. However, they could be considered in non-Euclidean spaces, resulting in new properties of (partially) invertible transformations.</p>
<p>Many other interesting information on flow-based models could be found in a fantastic review by (Papamakarios et al., 2019).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="References">References<a class="anchor-link" href="#References">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(Bachlechner et al., 2020) Bachlechner, T., Majumder, B. P., Mao, H. H., Cottrell, G. W., &amp; McAuley, J. (2020). Rezero is all you need: Fast convergence at large depth. arXiv preprint arXiv:2003.04887.</p>
<p>(Behrmann et al., 2019) Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., &amp; Jacobsen, J. H. (2019, May). Invertible residual networks. In International Conference on Machine Learning (pp. 573-582). PMLR.</p>
<p>(van den Berg et al., 2018) van den Berg, Rianne, Leonard Hasenclever, Jakub M. Tomczak, and Max Welling. "Sylvester normalizing flows for variational inference." In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, pp. 393-402. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.</p>
<p>(van den Berg et al., 2020) van den Berg, R., Gritsenko, A. A., Dehghani, M., SÃ¸nderby, C. K., &amp; Salimans, T. (2020). IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression. arXiv preprint arXiv:2006.12459.</p>
<p>(Brehmer &amp; Cranmer, 2020) Brehmer, J., &amp; Cranmer, K. (2020). Flows for simultaneous manifold learning and density estimation. Advances in Neural Information Processing Systems, 33.</p>
<p>(Chen et al., 2019) Chen, R. T., Behrmann, J., Duvenaud, D. K., &amp; Jacobsen, J. H. (2019). Residual flows for invertible generative modeling. In Advances in Neural Information Processing Systems (pp. 9916-9926).</p>
<p>(Dinh et al., 2016) Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. "Density estimation using real nvp." arXiv preprint arXiv:1605.08803 (2016).</p>
<p>(Ho et al., 2019a) Ho, J., Chen, X., Srinivas, A., Duan, Y., &amp; Abbeel, P. (2019). Flow++: Improving flow-based generative models with variational dequantization and architecture design. In International Conference on Machine Learning (pp. 2722-2730). PMLR.</p>
<p>(Ho et al., 2019b) Ho, J., Lohn, E., &amp; Abbeel, P. (2019). Compression with flows via local bits-back coding. arXiv preprint arXiv:1905.08500.</p>
<p>(Hoogeboom et al., 2019) Hoogeboom, E., Peters, J. W., Berg, R. V. D., &amp; Welling, M. (2019). Integer discrete flows and lossless compression. arXiv preprint arXiv:1905.07376.</p>
<p>(Hoogeboom et al., 2021) Hoogeboom, E., Cohen, T. S., &amp; Tomczak, J. M. (2020). Learning Discrete Distributions by Dequantization. AABI 2021</p>
<p>(Kingma et al., 2016) Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016). Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 4743-4751.</p>
<p>(Kingma &amp; Dhariwal, 2018) Kingma, D. P., &amp; Dhariwal, P. (2018). GLOW: generative flow with invertible 1Ã 1 convolutions. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 10236-10245).</p>
<p>(Papamakarios et al., 2019) Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., &amp; Lakshminarayanan, B. (2019). Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762.</p>
<p>(Perugachi-Diaz et al., 2021) Perugachi-Diaz, Y., Tomczak, J. M., &amp; Bhulai, S. (2021). Invertible DenseNets. AABI 2021</p>
<p>(Rezende &amp; Mohamed, 2015) Rezende, D., &amp; Mohamed, S. (2015). Variational Inference with Normalizing Flows. In International Conference on Machine Learning (pp. 1530-1538).</p>
<p>(Rippel &amp; Adams, 2013) Rippel, O., &amp; Adams, R. P. (2013). High-dimensional probability estimation with deep density models. arXiv</p>
<p>(Stypulkowski et al., 2020) Stypulkowski, M., Kania, K., Zamorski, M., Zieba, M., Trzcinski, T., &amp; Chorowski, J. (2020). Representing Point Clouds with Generative Conditional Invertible Flow Networks. arXiv preprint arXiv:2010.11087.</p>
<p>(Theis et al., 2016) Theis, L., Oord, A. V. D., &amp; Bethge, M. (2016). A note on the evaluation of generative models. ICLR 2016</p>
<p>(Tomczak &amp; Welling, 2016) Tomczak, J. M., &amp; Welling, M. (2016). Improving variational auto-encoders using householder flow. arXiv preprint arXiv:1611.09630.</p>
<p>(Tomczak &amp; Welling, 2017) Tomczak, J. M., &amp; Welling, M. (2017). Improving variational auto-encoders using convex combination linear inverse autoregressive flow. arXiv preprint arXiv:1706.02326.</p>
<p>(Tomczak, 2020) Tomczak, J. M. (2020). General Invertible Transformations for Flow-based Generative Modeling. arXiv preprint arXiv:2011.15056.</p>
<p>(Winkler et al., 2019) Winkler, C., Worrall, D., Hoogeboom, E., &amp; Welling, M. (2019). Learning likelihoods with conditional normalizing flows. arXiv preprint arXiv:1912.00042.</p>
<p>(Wolf et al., 2021) Wolf, V., Lugmayr, A., Danelljan, M., Van Gool, L., &amp; Timofte, R. (2021). DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows. arXiv preprint arXiv:2101.05796.</p>

</div>
</div>
</div>
    </div>
  </div>
</body>

 


</html>
